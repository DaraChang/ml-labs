{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/train/\"\n",
    "TEST_DATA_PATH =  \"data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transform, you can add different transform methods and resize image to any size\n",
    "img_size = \n",
    "train_transform = transforms.Compose([\n",
    "                                    transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=TRAIN_DATA_PATH,transform=train_transform)\n",
    "\n",
    "# spilt data into train and validation and the total number of image is 4276. You can decide the number of images\n",
    "# you want to use to do training and validation.\n",
    "TOTAL_SIZE = len(os.listdir(TRAIN_DATA_PATH + \"/NORMAL\")) + len(\n",
    "    os.listdir(TRAIN_DATA_PATH + \"/INFECTED\")\n",
    ")\n",
    "\n",
    "\n",
    "# spilt your data into train and val\n",
    "ratio = \n",
    "train_len = round(TOTAL_SIZE * ratio)\n",
    "valid_len = round(TOTAL_SIZE * (1-ratio))\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, valid_len])\n",
    "\n",
    "# you can use different batch size\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=, shuffle=True,  num_workers=4)\n",
    "val_data_loader = data.DataLoader(val_dataset, batch_size=, shuffle=True,  num_workers=4)\n",
    "print(dataset)\n",
    "print(dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have written the function for you this time, but it's strongly recommended that you \n",
    "# understand how to do training and validation\n",
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, epoch, verbose=True):\n",
    "    model.train()\n",
    "    loss_avg = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # loss function, you can use other loss function if you want\n",
    "        loss   = F.cross_entropy(output, target)\n",
    "        loss_avg = loss.item()\n",
    "        \n",
    "        # do back propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        verbose_step = len(data_loader) // 10\n",
    "        if batch_idx % verbose_step == 0 and verbose:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "    return loss_avg / len(data_loader)\n",
    "\n",
    "def valid(model, data_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            valid_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item() \n",
    "\n",
    "        valid_loss /= len(data_loader.dataset)\n",
    "        print('\\nValid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            valid_loss, correct, len(data_loader.dataset),\n",
    "            100. * correct / len(data_loader.dataset)))\n",
    "    return float(correct) / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Build the model here ##########\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "    \n",
    "        super(ConvNet, self).__init__()\n",
    "    \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  implement your optimizer ###################################\n",
    "## yo can use any training methods if you want (ex:lr decay, weight decay.....)\n",
    "\n",
    "lr=\n",
    "optimizer = \n",
    "\n",
    "# start training\n",
    "epochs = \n",
    "acc = 0.0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train(model, train_data_loader, optimizer, epoch)\n",
    "    accuracy = valid(model, val_data_loader)\n",
    "    if accuracy > acc:\n",
    "        acc = accuracy\n",
    "        print(\"-------------saving model--------------\")\n",
    "        # save the model\n",
    "        torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                    transforms.ToTensor()\n",
    "                                    ])\n",
    "\n",
    "test_data = datasets.ImageFolder(root=TEST_DATA_PATH,transform=test_transform)\n",
    "test_data_loader  = data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load the model so that you don't need to train the model again\n",
    "test_model = torch.load(\"model.pth\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "        bs = test_data_loader.batch_size\n",
    "        result = []\n",
    "        for i, (data, target) in enumerate(test_data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            arr = pred.data.cpu().numpy()\n",
    "            for j in range(pred.size()[0]):\n",
    "                file_name = test_data.samples[i*bs+j][0].split('/')[-1]\n",
    "                result.append((file_name,pred[j].cpu().numpy()[0]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(test_model,test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write results to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('ID_result.csv','w') as f:\n",
    "    f.write('ID,label\\n')\n",
    "    for data in result:\n",
    "        f.write(data[0]+','+str(data[1])+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
